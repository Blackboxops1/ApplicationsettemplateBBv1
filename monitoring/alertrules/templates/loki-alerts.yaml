apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: loki-alerts
spec:
  groups:
    - name: LokiAlerts
      rules:
        - alert: LokiRequestErrors
          expr: 100 * sum(rate(loki_request_duration_seconds_count{status_code=~"5.."}[1m])) by (namespace, job, route) / sum(rate(loki_request_duration_seconds_count[1m])) by (namespace, job, route) > 10
          for: 15m
          labels:
            severity: critical
          annotations:
            summary: Loki request errors (instance {{ printf "{{ $labels.instance }}" }})
            description: "The {{ printf "{{ $labels.job }}" }} and {{ printf "{{ $labels.route }}" }} are experiencing errors\n  VALUE = {{ printf "{{ $value }}" }}\n  LABELS = {{ printf "{{ $labels }}" }}"
        - alert: LokiRequestLatency
          expr: (histogram_quantile(0.99, sum(rate(loki_request_duration_seconds_bucket{route!~"(?i).*tail.*"}[5m])) by (le)))  > 1
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: Loki request latency (instance {{ printf "{{ $labels.instance }}" }})
            description: "The {{ printf "{{ $labels.job }}" }} {{ printf "{{ $labels.route }}" }} is experiencing {{ printf "{{ printf \"%.2f\" $value }}" }}s 99th percentile latency\n  VALUE = {{ printf "{{ $value }}" }}\n  LABELS = {{ printf "{{ $labels }}" }}"
        - alert: LokiRequestPanic
          expr: sum(increase(loki_panic_total[10m])) by (namespace, job) > 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: Loki request panic (instance {{ printf "{{ $labels.instance }}" }})
            description: "The {{ printf "{{ $labels.job }}" }} is experiencing {{ printf "{{ printf \"%.2f\" $value }}" }}% increase of panics\n  VALUE = {{ printf "{{ $value }}" }}\n  LABELS = {{ printf "{{ $labels }}" }}"